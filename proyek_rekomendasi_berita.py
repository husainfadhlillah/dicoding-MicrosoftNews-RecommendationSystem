# -*- coding: utf-8 -*-
"""proyek_rekomendasi_berita.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RHg5Dr_5-A8O9gmTvLCtwZaRX1EI5RGK

# **Sistem Rekomendasi Berita Menggunakan Pendekatan Content-Based dan Collaborative Filtering**

- **Nama:** Muhammad Husain Fadhlillah
- **Email Student:** mc006d5y2343@student.devacademy.id
- **Cohort ID:** MC006D5Y2343

![dataset-cover.png](attachment:dataset-cover.png)

#### **Overview Proyek**
Proyek ini bertujuan untuk mengatasi masalah *information overload* di platform berita digital dengan membangun sistem rekomendasi yang dipersonalisasi. Dengan memanfaatkan dataset interaksi pengguna dan konten berita dari Microsoft News (MIND), proyek ini mengembangkan dan mengevaluasi dua model rekomendasi utama: *Content-Based Filtering* dan *Collaborative Filtering*. Tujuan akhirnya adalah untuk menyajikan daftar berita yang relevan kepada pengguna, sehingga meningkatkan pengalaman dan keterlibatan mereka dengan platform.

### **I. Persiapan Library**
"""

# Commented out IPython magic to ensure Python compatibility.
### Instalasi Library `wordcloud`
# %pip install wordcloud

"""* **Metode:** Instalasi paket menggunakan `pip` melalui *magic command* Jupyter.
* **Alasan Penggunaan:** Library `wordcloud` diperlukan untuk membuat visualisasi *word cloud* pada tahap *Exploratory Data Analysis* (EDA). Visualisasi ini membantu mengidentifikasi kata kunci yang paling sering muncul dalam judul berita.
* **Analisis:** Output `Requirement already satisfied` menunjukkan bahwa library `wordcloud` beserta dependensinya (seperti `numpy`, `pillow`, `matplotlib`) sudah terinstal di environment Anda. Peringatan (`WARNING`) yang muncul dapat diabaikan karena tidak menghalangi fungsionalitas library utama.
"""

# Untuk mengabaikan peringatan yang tidak relevan agar output lebih bersih
import warnings
warnings.filterwarnings('ignore')

# Library untuk analisis dan manipulasi data
import pandas as pd
import numpy as np
from collections import defaultdict

# Library untuk visualisasi data
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

# Library untuk pemodelan Content-Based Filtering
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Library untuk pemodelan Collaborative Filtering
# %pip install scikit-surprise -q
from surprise import Reader, Dataset, SVD
from surprise.model_selection import train_test_split as surprise_train_test_split

# Mengatur opsi tampilan pandas
pd.set_option('display.max_columns', None)

print("Semua library berhasil diimpor.")

"""##### **1. Metode yang Digunakan**

Tahap ini merupakan fondasi dari keseluruhan proyek, di mana kita menyiapkan semua "perkakas" atau *library* yang diperlukan. Metode yang diterapkan adalah **impor modul**, yaitu memuat berbagai *library* Python yang memiliki fungsionalitas spesifik ke dalam environment kerja.

Library utama yang diimpor dikelompokkan berdasarkan fungsinya:
* **Manipulasi Data dan Numerik:** `pandas` dan `numpy`.
* **Visualisasi Data:** `matplotlib`, `seaborn`, dan `wordcloud`.
* **Pemodelan Content-Based:** `TfidfVectorizer` dan `cosine_similarity` dari `scikit-learn`.
* **Pemodelan Collaborative Filtering:** `Reader`, `Dataset`, dan `SVD` dari `surprise`.
* **Konfigurasi Environment:** `warnings` dan `pd.set_option`.

##### **2. Alasan Penggunaan Metode**

Pemilihan *library* ini sangat relevan dan krusial karena masing-masing mendukung tahapan spesifik dalam alur kerja proyek *machine learning*, mulai dari pemahaman data hingga evaluasi model.

* **`pandas` dan `numpy`**: Dipilih karena merupakan standar industri untuk analisis data di Python. `pandas` sangat esensial untuk membaca file dataset (`.tsv`), membersihkan, dan mentransformasikannya ke dalam struktur DataFrame yang mudah dimanipulasi. `numpy` mendukung operasi matematis yang efisien pada data numerik.
* **`matplotlib`, `seaborn`, `wordcloud`**: Digunakan untuk tahap *Exploratory Data Analysis* (EDA). Visualisasi adalah cara paling efektif untuk memahami distribusi data, menemukan pola, dan mendapatkan wawasan awal. Pemilihan ketiga library ini memungkinkan pembuatan berbagai jenis plot, dari grafik statistik standar hingga visualisasi teks yang lebih kompleks seperti *word cloud*, yang sangat penting untuk memahami data konten berita.
* **`scikit-learn`**: Dipilih karena merupakan *toolkit* terlengkap untuk *machine learning* di Python. `TfidfVectorizer` digunakan untuk mengubah data teks (judul dan kategori berita) menjadi representasi vektor numerik, sebuah langkah fundamental dalam *Content-Based Filtering*. `cosine_similarity` kemudian digunakan sebagai metrik untuk mengukur kemiripan antar vektor berita tersebut.
* **`surprise`**: Dipilih karena merupakan *library* yang didesain khusus untuk membangun dan menganalisis sistem rekomendasi. Menggunakan `surprise` lebih efisien dan andal dibandingkan membangun algoritma *Collaborative Filtering* dari awal. Algoritma **SVD** (*Singular Value Decomposition*) secara spesifik dipilih karena kemampuannya yang sangat baik dalam menemukan pola laten (*latent patterns*) dari data interaksi pengguna-item, yang cocok untuk data implisit seperti riwayat klik.

##### **3. Insight dan Hasil yang Didapat**

* **Analisis Output:** Output yang dihasilkan adalah sebuah pesan konfirmasi sederhana: `"Semua library berhasil diimpor."`.
* **Insight:** Meskipun terlihat sepele, output ini adalah **pemeriksaan fundamental pertama yang paling penting**. Ini mengonfirmasi bahwa *environment* kerja telah disiapkan dengan benar dan semua dependensi yang diperlukan untuk menjalankan setiap bagian dari notebook—mulai dari pemuatan data, EDA, hingga pemodelan yang kompleks—telah tersedia. Tidak adanya pesan error `ModuleNotFoundError` menunjukkan bahwa proyek ini dapat direproduksi (*reproducible*) dengan baik di lingkungan yang memiliki paket-paket tersebut. Dengan kata lain, fondasi teknis dari proyek ini sudah kokoh, dan kita dapat melanjutkan ke tahap analisis data dengan percaya diri.

## **II. Data Understanding**
Tahap ini berfokus pada pemuatan dan pemahaman awal terhadap dataset. Kita akan menggunakan dataset MIND-small dari Kaggle.

Sumber Dataset: **MIND: Microsoft News Recommendation Dataset** [https://www.kaggle.com/datasets/arashnic/mind-news-dataset]

### **Memuat Dataset**
Kita akan memuat file `news.tsv` dan `behaviors.tsv` ke dalam DataFrame pandas.
"""

# Fungsi ini dirancang untuk memuat data dari file TSV (Tab-Separated Values) dengan aman.
# Menggunakan blok try-except adalah praktik terbaik untuk menangani error jika file tidak ditemukan.
def load_data(file_path, column_names):
    """
    Memuat data dari file TSV ke dalam DataFrame pandas.

    Args:
        file_path (str): Path ke file tsv.
        column_names (list): Daftar nama kolom untuk DataFrame.

    Returns:
        DataFrame: DataFrame yang berisi data dari file.
    """
    try:
        # pd.read_csv digunakan untuk membaca file. sep='\t' menunjukkan pemisah antar kolom adalah tab.
        # names=column_names memberikan nama untuk setiap kolom.
        df = pd.read_csv(file_path, sep='\t', names=column_names)
        print(f"File '{file_path}' berhasil dimuat.")
        return df
    except FileNotFoundError:
        print(f"Error: File '{file_path}' tidak ditemukan. Pastikan path file sudah benar.")
        return None

# Path ke file dataset yang akan digunakan (disimpan dalam subfolder MINDsmall_train).
path_to_news = './MINDsmall_train/news.tsv'
path_to_behaviors = './MINDsmall_train/behaviors.tsv'

# Mendefinisikan nama kolom untuk masing-masing file berdasarkan dokumentasi dataset.
news_cols = ['News_ID', 'Category', 'SubCategory', 'Title', 'Abstract', 'URL', 'Title_Entities', 'Abstract_Entities']
behaviors_cols = ['Impression_ID', 'User_ID', 'Time', 'History', 'Impressions']

# Memanggil fungsi load_data untuk memuat kedua file ke dalam DataFrame.
df_news = load_data(path_to_news, news_cols)
df_behaviors = load_data(path_to_behaviors, behaviors_cols)

"""##### **1. Metode yang Digunakan**

* **Fungsi Kustom dengan Error Handling**: Sebuah fungsi Python kustom (`load_data`) dibuat untuk membungkus logika pemuatan data. Di dalamnya, digunakan blok `try-except` untuk menangani `FileNotFoundError`, yang merupakan praktik kode yang tangguh (*robust*).
* **Pandas `read_csv`**: Metode inti yang digunakan adalah fungsi `read_csv` dari *library* Pandas. Fungsi ini sangat fleksibel dan digunakan di sini untuk membaca file dengan format `.tsv` (*Tab-Separated Values*).
* **Parameter Spesifik**: Dua parameter penting digunakan: `sep='\t'` untuk menentukan bahwa pemisah kolom adalah karakter *tab*, dan `names=column_names` untuk secara eksplisit menetapkan nama kolom pada DataFrame yang dihasilkan.

##### **2. Alasan Penggunaan Metode**

* **Modularitas dan Reusabilitas**: Membuat fungsi `load_data` adalah pilihan desain yang baik. Daripada menulis kode pemuatan data dua kali (untuk `news.tsv` dan `behaviors.tsv`), kita mendefinisikannya sekali dan memanggilnya sesuai kebutuhan. Ini membuat notebook lebih bersih, mudah dibaca, dan mudah dikelola.
* **Ketahanan Kode (*Robustness*)**: Penggunaan `try-except` sangat penting. Ini memastikan bahwa jika file dataset tidak ada di lokasi yang ditentukan, program tidak akan berhenti dengan *error traceback* yang panjang. Sebaliknya, ia akan menampilkan pesan yang informatif ("Error: File ... tidak ditemukan.") dan melanjutkan eksekusi sel berikutnya. Ini adalah ciri kode berkualitas produksi.
* **Akurasi Parsing Data**: Dataset MIND menggunakan format `.tsv`, bukan `.csv` (Comma-Separated). Oleh karena itu, penggunaan `sep='\t'` adalah **krusial** untuk memastikan data diparsing dengan benar ke dalam kolom-kolom yang terpisah. Tanpa ini, seluruh baris akan dibaca sebagai satu kolom tunggal.
* **Keterbacaan Data**: File sumber tidak menyertakan baris *header*. Dengan menyediakan nama kolom secara eksplisit melalui parameter `names`, kita langsung mendapatkan DataFrame yang terstruktur dan mudah dipahami. Ini jauh lebih baik daripada bekerja dengan nama kolom default (misalnya, 0, 1, 2, ...), yang akan menyulitkan analisis di tahap selanjutnya.

##### **3. Insight dan Hasil yang Didapat**

* **Analisis Output**: Output yang ditampilkan adalah dua pesan konfirmasi: `"File './MINDsmall_train/news.tsv' berhasil dimuat."` dan `"File './MINDsmall_train/behaviors.tsv' berhasil dimuat."`.
* **Insight**: Output ini adalah **validasi pertama yang sukses** dalam alur kerja proyek. Ini mengonfirmasi beberapa hal penting:
    1.  **Struktur Proyek Benar**: File dataset ditempatkan dengan benar di dalam direktori `MINDsmall_train` relatif terhadap lokasi notebook.
    2.  **Aksesibilitas Data**: Program memiliki izin untuk membaca file dari disk.
    3.  **Inisialisasi DataFrame Berhasil**: Data dari kedua file kini telah berhasil dimuat ke dalam dua variabel DataFrame yang terpisah: `df_news` (berisi metadata konten berita) dan `df_behaviors` (berisi data interaksi pengguna).

### **Informasi Dasar Dataset**
Melihat beberapa baris pertama, informasi, dan statistik deskriptif dari data.
"""

# Menampilkan 5 baris pertama dari data berita
if df_news is not None:
    print("Contoh Data Berita (df_news):")
    display(df_news.head())

# Menampilkan 5 baris pertama dari data perilaku
if df_behaviors is not None:
    print("\nContoh Data Perilaku (df_behaviors):")
    display(df_behaviors.head())

"""##### **1. Metode yang Digunakan**

* **Pandas `.head()`**: Metode ini digunakan untuk mengambil dan menampilkan 5 baris pertama dari sebuah DataFrame. Ini adalah langkah standar dan fundamental dalam tahap *Data Understanding*.
* **Fungsi `display()`**: Alih-alih menggunakan `print()`, `display()` digunakan karena ini adalah fungsi dari environment IPython (Jupyter) yang merender DataFrame menjadi tabel HTML yang rapi dan mudah dibaca. Ini meningkatkan kualitas presentasi notebook.
* **Pemeriksaan Kondisional (`if df is not None`)**: Kode ini dibungkus dalam blok `if` untuk memastikan bahwa `.head()` hanya akan dijalankan jika DataFrame berhasil dimuat pada sel sebelumnya. Ini adalah praktik *defensive programming* yang baik.

##### **2. Alasan Penggunaan Metode**

Tujuan utama dari sel ini adalah melakukan **verifikasi visual** atau *sanity check* terhadap data yang baru saja dimuat. Alasan penggunaan `.head()` sangat krusial:

* **Konfirmasi Struktur Data**: Dengan melihat beberapa baris pertama, kita dapat langsung memastikan bahwa data telah diparsing dengan benar ke dalam kolom-kolom yang sesuai, dan nama kolom yang kita definisikan sebelumnya telah diterapkan dengan benar.
* **Pemahaman Format Fitur**: Ini memungkinkan kita untuk menginspeksi format data di setiap kolom. Misalnya, kita dapat melihat bahwa `History` adalah serangkaian `News_ID` yang dipisahkan spasi, dan `Impressions` memiliki format yang lebih kompleks dengan sufiks `-1` (diklik) dan `-0` (tidak diklik). Pemahaman ini sangat penting untuk merencanakan langkah-langkah *Data Preparation* selanjutnya.
* **Identifikasi Awal Tipe Data**: Kita mendapatkan gambaran awal tentang tipe data—apakah itu teks, numerik, atau format kompleks seperti JSON yang tersimpan sebagai *string* (pada kolom `Title_Entities`).

##### **3. Insight dan Hasil yang Didapat**

* **Analisis Output `df_news`**:
    * Tabel ini adalah **sumber data untuk model Content-Based**. Kolom `Category`, `SubCategory`, dan `Title` adalah fitur konten utama yang akan digunakan.
    * **Insight**: Kolom `Title_Entities` dan `Abstract_Entities` berisi data terstruktur dalam format JSON. Ini adalah fitur yang sangat kaya (entitas seperti nama orang, lokasi, organisasi) yang telah diekstrak sebelumnya. Meskipun proyek ini hanya akan menggunakan judul dan kategori, keberadaan fitur ini menunjukkan potensi untuk pengembangan model yang lebih canggih di masa depan.

* **Analisis Output `df_behaviors`**:
    * Tabel ini adalah **sumber data untuk model Collaborative Filtering**. `User_ID` akan menjadi identitas pengguna, dan kolom `History` serta `Impressions` adalah representasi dari perilaku mereka.
    * **Insight**: Kolom `History` dan `Impressions` berada dalam format "mentah" (string panjang) dan belum siap untuk pemodelan. Kolom `History` berisi riwayat klik implisit yang akan menjadi dasar profil pengguna. Kolom `Impressions` bahkan lebih kaya karena mengandung informasi **umpan balik negatif implisit** (berita yang ditampilkan tetapi tidak diklik, ditandai dengan `-0`), yang sangat berharga untuk melatih model rekomendasi yang lebih cerdas.
"""

# Menampilkan informasi ringkas tentang DataFrame berita
if df_news is not None:
    print("Informasi df_news:")
    df_news.info()

# Menampilkan informasi ringkas tentang DataFrame perilaku
if df_behaviors is not None:
    print("\nInformasi df_behaviors:")
    df_behaviors.info()

"""##### **1. Metode yang Digunakan**

* **Pandas `.info()`**: Metode utama yang digunakan adalah `.info()` dari library Pandas. Fungsi ini sangat efisien karena memberikan ringkasan teknis yang padat tentang sebuah DataFrame, termasuk jumlah baris, jumlah kolom, jumlah nilai non-null per kolom, tipe data setiap kolom, dan penggunaan memori.

##### **2. Alasan Penggunaan Metode**

Langkah ini merupakan bagian krusial dari *Data Understanding* yang melengkapi inspeksi visual (`.head()`). Penggunaan `.info()` sangat penting karena beberapa alasan:

* **Verifikasi Skala Data**: Memberikan angka pasti mengenai ukuran dataset (jumlah baris dan kolom). Ini membantu kita memahami skala masalah dan mengantisipasi kebutuhan komputasi.
* **Deteksi Cepat Nilai yang Hilang (*Missing Values*)**: Ini adalah salah satu kegunaan terpenting dari `.info()`. Dengan membandingkan `Non-Null Count` dengan total entri (`RangeIndex`), kita dapat dengan cepat mengidentifikasi kolom mana saja yang memiliki data tidak lengkap. Informasi ini sangat vital untuk merencanakan strategi pembersihan data di tahap *Data Preparation*.
* **Validasi Tipe Data**: Memastikan setiap kolom memiliki tipe data (`Dtype`) yang benar. Sebagai contoh, kolom yang seharusnya numerik namun terbaca sebagai `object` (string) dapat menyebabkan error pada perhitungan matematis.
* **Efisiensi Memori**: Informasi `memory usage` memberikan gambaran tentang seberapa besar dataset dimuat ke dalam RAM, yang berguna untuk manajemen sumber daya pada dataset berskala besar.

##### **3. Insight dan Hasil yang Didapat**

Analisis output `.info()` memberikan beberapa wawasan teknis yang sangat berharga:

* **Analisis `df_news`**:
    * **Skala**: Dataset berita terdiri dari **51,282 artikel**.
    * **Kualitas Data (Nilai Hilang)**: Ditemukan adanya nilai yang hilang pada kolom `Abstract` (48,616 dari 51,282), `Title_Entities`, dan `Abstract_Entities`. Ini menandakan bahwa tidak semua berita memiliki deskripsi abstrak yang lengkap. Untuk model kita yang berfokus pada `Title` dan `Category`, ini bukan masalah besar, namun ini adalah catatan penting tentang kualitas data.
    * **Tipe Data**: Semua kolom bertipe `object`, yang mengindikasikan bahwa data di dalamnya adalah *string*. Ini sesuai dengan ekspektasi untuk fitur-fitur seperti judul dan kategori.

* **Analisis `df_behaviors`**:
    * **Skala**: Dataset perilaku mencakup **156,965 sesi interaksi pengguna**.
    * **Kualitas Data (Nilai Hilang)**: Kolom `History` memiliki **153,727 nilai non-null**, yang lebih sedikit dari total entri.
    * **Insight**: Adanya nilai yang hilang di kolom `History` adalah sebuah temuan penting. Ini sangat mungkin merepresentasikan **pengguna baru** yang belum memiliki riwayat interaksi saat sesi mereka dicatat. Fenomena ini adalah manifestasi nyata dari **masalah *cold start***, sebuah tantangan klasik dalam sistem rekomendasi di mana sistem kesulitan memberikan rekomendasi kepada pengguna baru.
    * **Tipe Data**: Kolom `Time` bertipe `object`. Ini berarti jika ada kebutuhan untuk analisis berbasis waktu (misalnya, rekomendasi berita tren), kolom ini harus diubah terlebih dahulu ke format `datetime`.

### **Exploratory Data Analysis (EDA)**
Melakukan analisis data eksplorasi untuk memahami distribusi data.

#### **Distribusi Kategori Berita**
"""

if df_news is not None:
    plt.figure(figsize=(12, 6))
    sns.countplot(y='Category', data=df_news, order=df_news['Category'].value_counts().index)
    plt.title('Distribusi Kategori Berita')
    plt.xlabel('Jumlah Artikel')
    plt.ylabel('Kategori')
    plt.show()

"""##### **1. Metode yang Digunakan**

* **Visualisasi Data**: Metode utama yang diterapkan di sini adalah visualisasi data, yang merupakan bagian inti dari *Exploratory Data Analysis* (EDA).
* **Count Plot (Grafik Batang)**: Secara spesifik, `seaborn.countplot` digunakan untuk secara otomatis menghitung frekuensi setiap entri unik dalam kolom `Category` dan menampilkannya dalam bentuk grafik batang.
* **Pengurutan Data**: Digunakan metode `value_counts().index` dari Pandas untuk mengurutkan kategori berdasarkan jumlah artikelnya secara menurun. Hasilnya kemudian dimasukkan ke dalam parameter `order` pada `countplot`, sehingga grafik yang dihasilkan lebih terstruktur dan mudah diinterpretasikan.

##### **2. Alasan Penggunaan Metode**

Tujuan dari sel ini adalah untuk memahami komposisi dataset berdasarkan fitur konten yang paling fundamental, yaitu kategori berita.

* **Mengidentifikasi Pola Dominan**: `countplot` adalah metode visual yang paling efektif dan intuitif untuk melihat kategori mana yang paling dominan dan mana yang paling sedikit jumlahnya. Ini memberikan gambaran cepat tentang fokus utama konten dalam dataset.
* **Mendeteksi Ketidakseimbangan Data**: Visualisasi ini sangat penting untuk mendeteksi apakah dataset seimbang atau tidak. Ketidakseimbangan data adalah fenomena umum yang dapat mempengaruhi performa model *machine learning* dan perlu diidentifikasi sejak awal.
* **Meningkatkan Keterbacaan**: Mengurutkan data sebelum di-plot adalah praktik terbaik. Tanpa pengurutan, grafik akan terlihat acak dan sulit untuk menarik kesimpulan. Dengan mengurutkannya, kita dapat dengan mudah melihat peringkat popularitas setiap kategori.

##### **3. Insight dan Hasil yang Didapat**

* **Analisis Visualisasi**: Grafik batang dengan jelas menunjukkan jumlah artikel pada sumbu-X untuk setiap kategori berita pada sumbu-Y. Batang diurutkan dari yang terpanjang (paling banyak) di atas hingga yang terpendek di bawah.

* **Insight #1: Dominasi Kategori Tertentu**: Terlihat jelas bahwa dataset ini sangat didominasi oleh kategori **'news'**, dengan jumlah artikel lebih dari 20.000. Kategori **'sports'** menempati urutan kedua dengan jumlah yang jauh lebih sedikit, yaitu sekitar 6.000-7.000 artikel. Kategori lainnya seperti 'lifestyle' dan 'entertainment' memiliki jumlah yang lebih kecil lagi.

* **Insight #2: Distribusi yang Sangat Miring (*Skewed*)**: Distribusi kategori berita ini **sangat tidak seimbang**. Terdapat beberapa kategori "superstar" (seperti 'news' dan 'sports') yang jumlahnya jauh melampaui kategori-kategori lainnya. Fenomena ini dikenal sebagai *long-tail distribution*.

#### **Distribusi Panjang Riwayat Baca Pengguna**
Visualisasi ini menunjukkan seberapa aktif para pengguna dalam membaca berita.
"""

# Cek dulu apakah df_behaviors ada dan kolom History tidak kosong
if 'df_behaviors' in globals() and not df_behaviors['History'].isnull().all():
    # Menghitung panjang riwayat baca untuk setiap entri yang tidak null
    history_lengths = df_behaviors['History'].dropna().apply(lambda x: len(x.split()))

    plt.figure(figsize=(12, 6))
    sns.histplot(history_lengths, bins=50, kde=True)
    plt.title('Distribusi Panjang Riwayat Baca Pengguna')
    plt.xlabel('Jumlah Berita dalam Riwayat')
    plt.ylabel('Frekuensi')
    plt.xlim(0, 100) # Batasi sumbu x agar lebih mudah dibaca
    plt.show()

"""##### **1. Metode yang Digunakan**
* **Rekayasa Fitur (*Feature Engineering*)**: Sebuah fitur baru dihitung sebelum visualisasi. Dengan menggunakan fungsi `pandas.apply` dan `lambda`, kolom `History` (yang berisi string) ditransformasi menjadi data numerik `history_lengths`.
* **Visualisasi Data (Histogram)**: Metode utamanya adalah pembuatan **histogram** menggunakan `seaborn.histplot`. Penambahan `kde=True` memberikan kurva kepadatan yang memperhalus bentuk distribusi.
* **Pembatasan Sumbu (*Axis Limiting*)**: Teknik `plt.xlim(0, 100)` digunakan untuk memfokuskan visualisasi pada rentang data yang paling relevan.

##### **2. Alasan Penggunaan Metode**
Tujuan dari sel ini adalah untuk memahami tingkat keaktifan pengguna.
* **Mengukur Keterlibatan Pengguna**: Menghitung panjang riwayat adalah cara langsung untuk mengukur seberapa aktif seorang pengguna. Data ini sangat penting untuk model *Collaborative Filtering* yang belajar dari pola interaksi.
* **Memahami Distribusi**: Histogram adalah pilihan terbaik untuk menjawab pertanyaan seperti: "Berapa banyak berita yang biasanya dibaca oleh seorang pengguna?"
* **Fokus pada Mayoritas**: Penggunaan `plt.xlim` adalah keputusan analisis yang tepat. Tanpa ini, beberapa pengguna yang sangat aktif akan membuat distribusi mayoritas pengguna sulit dianalisis.

##### **3. Insight dan Hasil yang Didapat**
* **Analisis Visualisasi**: Grafik menunjukkan puncak yang sangat tinggi di sisi kiri dan menurun secara drastis ke arah kanan. Ini adalah ciri khas dari **distribusi *right-skewed*** atau **distribusi *long-tail***.
* **Insight #1: Dominasi Pengguna Kasual**: Mayoritas pengguna adalah **pengguna kasual** dengan riwayat baca yang sangat singkat, umumnya kurang dari 20 artikel. Puncak tertinggi pada grafik berada di angka yang sangat rendah (sekitar 0-5 artikel).
* **Insight #2: Keberadaan *Power Users***: "Ekor" panjang di sisi kanan grafik menunjukkan adanya sekelompok kecil ***power users*** yang sangat aktif.

#### **Word Cloud dari Judul Berita**
Untuk mendapatkan gambaran tentang topik utama yang dibahas dalam dataset, kita akan membuat Word Cloud dari semua judul berita. Ini membantu kita memahami "konten" dari berita yang akan digunakan oleh model Content-Based Filtering.
"""

# Cek apakah df_news ada
if 'df_news' in globals():
    # Menggabungkan semua judul berita menjadi satu teks besar
    all_titles = ' '.join(df_news['Title'].dropna())

    # Membuat objek WordCloud
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_titles)

    # Menampilkan WordCloud
    plt.figure(figsize=(15, 7))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off') # Menghilangkan sumbu
    plt.title('Word Cloud dari Judul Berita', size=20)
    plt.show()

"""##### **1. Metode yang Digunakan**
* **Visualisasi Teks (Word Cloud)**: Metode utama yang digunakan adalah pembuatan *Word Cloud* dengan menggunakan *library* `wordcloud`. Ini adalah teknik visualisasi yang populer untuk data teks tidak terstruktur.
* **Agregasi Teks**: Sebelum visualisasi, dilakukan langkah pra-pemrosesan sederhana dengan menggabungkan semua judul berita menjadi satu teks tunggal menggunakan metode `' '.join()`. Ini adalah format input yang diperlukan oleh *library* `wordcloud`.

##### **2. Alasan Penggunaan Metode**
Tujuan dari sel ini adalah untuk mendapatkan pemahaman kualitatif dan tematik dari data konten berita, yang melengkapi analisis kuantitatif dari grafik sebelumnya.

* **Identifikasi Topik Utama secara Cepat**: *Word cloud* adalah cara yang sangat efektif dan intuitif untuk mengidentifikasi kata-kata kunci dan topik yang paling sering dibicarakan dalam kumpulan data teks yang besar. Kata yang lebih besar secara visual langsung menarik perhatian, membuatnya lebih mudah diinterpretasikan daripada melihat tabel frekuensi kata.
* **Memahami "Rasa" Konten**: Visualisasi ini memberikan "rasa" atau nuansa dari konten yang akan dianalisis oleh model *Content-Based Filtering*. Ini membantu kita memahami "bahan baku" yang akan digunakan model untuk menentukan kemiripan antar berita.
* **Pelengkap EDA**: *Word cloud* melengkapi grafik distribusi kategori. Sementara grafik kategori menunjukkan pembagian topik di level tinggi, *word cloud* memberikan detail di level kata kunci, memberikan wawasan yang lebih kaya.

##### **3. Insight dan Hasil yang Didapat**
* **Analisis Visualisasi**: *Word cloud* yang dihasilkan menampilkan kata-kata dengan berbagai ukuran. Kata-kata yang paling besar dan dominan adalah **`Trump`**, **`says`**, **`new`**, dan **`US`**. Kata-kata berukuran sedang mencakup **`show`**, **`game`**, **`Health`**, dan **`Foods`**.

* **Insight #1: Konfirmasi Topik Dominan**: Visualisasi ini mengonfirmasi temuan dari grafik distribusi kategori. Kata-kata besar seperti **`Trump`** dan **`US`** menunjukkan fokus kuat pada berita politik Amerika. Demikian pula, kata **`Health`**, **`Foods`**, **`show`**, dan **`game`** secara langsung berhubungan dengan kategori dominan yang sudah diidentifikasi sebelumnya (`health`, `foodanddrink`, `entertainment`, `sports`).

* **Insight #2: Identifikasi Kata Umum (Stop Words Kontekstual)**: Kata **`says`** muncul sebagai salah satu kata terbesar. Meskipun sangat sering muncul dalam judul berita ("*X says Y*"), kata ini tidak memberikan banyak informasi tentang *topik* spesifik berita tersebut. Dalam konteks analisis teks, kata seperti ini bisa dianggap sebagai "*stop word*" yang dapat dihilangkan pada tahap pra-pemrosesan yang lebih lanjut untuk mendapatkan wawasan yang lebih bersih tentang kata kunci yang benar-benar informatif.

## **III. Data Preparation**
Menyiapkan data agar siap untuk proses pemodelan.

### **Membersihkan dan Menggabungkan Data**
Kita akan membersihkan data dari nilai null dan menggabungkan informasi yang relevan. Langkah ini krusial karena:
- **Pembersihan Data:** Memastikan tidak ada data yang hilang pada fitur-fitur kunci (`Title`, `Category`) yang dapat menyebabkan error atau hasil yang tidak akurat.
- **Sampling:** Mengambil sampel dari data perilaku dilakukan untuk mempercepat proses komputasi, terutama pada mesin dengan sumber daya terbatas, tanpa kehilangan representasi umum dari pola interaksi pengguna.
- **Penggabungan Data:** Menggabungkan data interaksi (`df_interaction`) dengan detail berita (`df_news_cleaned`) diperlukan agar kita memiliki informasi konten untuk setiap berita yang diklik pengguna.
"""

if df_news is not None and df_behaviors is not None:
    # Membersihkan data berita dari nilai null pada kolom penting
    df_news_cleaned = df_news.dropna(subset=['Title', 'Category'])
    print(f"Jumlah data berita setelah dibersihkan: {len(df_news_cleaned)}")

    # Mengambil sampel data perilaku untuk mempercepat proses (opsional, bisa di-skip jika komputasi kuat)
    df_behaviors_sample = df_behaviors.sample(n=50000, random_state=42)

    # Memproses kolom 'History' untuk mendapatkan interaksi user-item
    user_history_list = []
    for index, row in df_behaviors_sample.iterrows():
        if isinstance(row['History'], str):
            history_ids = row['History'].split()
            for news_id in history_ids:
                user_history_list.append({'User_ID': row['User_ID'], 'News_ID': news_id})

    df_interaction = pd.DataFrame(user_history_list)
    print("Contoh data interaksi dari 'History':")
    display(df_interaction.head())

    # Menggabungkan data interaksi dengan data berita
    df_full_interaction = pd.merge(df_interaction, df_news_cleaned, on='News_ID', how='inner')
    print("Contoh data interaksi yang sudah digabungkan dengan detail berita:")
    display(df_full_interaction.head())

"""##### **1. Metode yang Digunakan**
* **Pembersihan Data**: Metode `.dropna()` dari Pandas digunakan untuk menghapus baris yang memiliki nilai hilang (*null*) pada kolom-kolom krusial (`Title`, `Category`).
* **Pengambilan Sampel (*Sampling*)**: Metode `.sample()` digunakan untuk mengambil sampel acak sebanyak 50.000 baris dari data perilaku.
* **Rekayasa Fitur (*Feature Engineering*)**: Iterasi `for` digunakan untuk memproses kolom `History` yang berisi string, memecahnya menjadi ID berita individual, dan menyusunnya kembali menjadi format *long* (satu baris per interaksi).
* **Penggabungan Data**: Metode `pd.merge()` digunakan untuk menggabungkan dua DataFrame (`df_interaction` dan `df_news_cleaned`) berdasarkan kolom kunci yang sama (`News_ID`).

##### **2. Alasan Penggunaan Metode**
Langkah-langkah dalam sel ini sangat penting untuk mentransformasi data mentah menjadi format yang bersih dan terstruktur yang dapat digunakan oleh model.
* **Menjamin Kualitas Fitur**: `.dropna()` digunakan untuk memastikan bahwa fitur-fitur yang akan menjadi inti dari model *Content-Based* (`Title` dan `Category`) tidak memiliki data yang hilang, yang dapat menyebabkan error atau hasil yang tidak akurat.
* **Efisiensi Komputasi**: Dataset perilaku asli sangat besar. `.sample()` adalah teknik praktis untuk mengurangi ukuran data secara signifikan sehingga proses pelatihan dan evaluasi model dapat berjalan lebih cepat, terutama pada mesin dengan sumber daya terbatas. Penggunaan `random_state` memastikan bahwa sampel yang diambil selalu sama, sehingga hasil proyek dapat direproduksi.
* **Menyiapkan Data untuk Model**: Model *Collaborative Filtering* memerlukan data dalam format "satu interaksi per baris". Oleh karena itu, memproses kolom `History` dari format *wide* ke format *long* adalah langkah yang wajib dilakukan.
* **Mengintegrasikan Data**: `pd.merge()` sangat penting untuk menyatukan data interaksi dengan data konten. Ini memungkinkan kita untuk menganalisis dan mengevaluasi kedua jenis model pada dataset yang terintegrasi.

##### **3. Insight dan Hasil yang Didapat**
* **Analisis Output**:
    * Jumlah berita setelah pembersihan tetap **51.282**, yang mengindikasikan tidak ada berita dengan judul atau kategori kosong dalam dataset ini.
    * Tabel `df_interaction` menunjukkan hasil transformasi yang sukses. Data yang tadinya berupa string panjang di kolom `History` kini menjadi DataFrame yang rapi dengan kolom `User_ID` dan `News_ID`, yang merupakan format ideal untuk model berbasis interaksi.
    * Tabel `df_full_interaction` menunjukkan hasil penggabungan yang berhasil. Sekarang, setiap baris tidak hanya berisi `User_ID` dan `News_ID`, tetapi juga diperkaya dengan informasi konten dari berita tersebut (seperti `Category` dan `Title`).

* **Insight**:
    * **Pentingnya Transformasi Data**: Sel ini secara efektif menunjukkan betapa krusialnya tahap persiapan data. Data mentah hampir tidak pernah siap pakai. Transformasi dari format *wide* ke *long* adalah langkah kunci yang memungkinkan analisis lebih lanjut dan pemodelan.
    * **Kesiapan Data untuk Dua Pendekatan**: Dengan DataFrame `df_full_interaction`, kita sekarang memiliki dataset terpadu yang siap digunakan untuk kedua pendekatan. Untuk *Content-Based*, kita bisa memfilter berdasarkan `News_ID` yang dibaca pengguna dan mencari berita lain dengan `Category` atau `Title` yang mirip. Untuk *Collaborative Filtering*, kita memiliki pasangan `User_ID` dan `News_ID` yang dibutuhkan untuk membangun matriks interaksi.
    * **Keputusan Sampling**: Pengambilan sampel sebanyak 50.000 interaksi adalah keputusan pragmatis yang menyeimbangkan antara kecepatan komputasi dan representasi data. Ini adalah pertimbangan penting dalam proyek dunia nyata ketika berhadapan dengan data berskala besar.

## **IV. Modeling: Content-Based Filtering**
Membuat model yang merekomendasikan berita berdasarkan kemiripan konten.
"""

# Mengambil data berita unik untuk model content-based
df_content = df_news_cleaned.drop_duplicates(subset=['News_ID']).reset_index(drop=True)

# Membuat fitur 'content' dengan menggabungkan judul dan kategori
df_content['content'] = df_content['Title'] + ' ' + df_content['Category']

# Inisialisasi TfidfVectorizer
tfidf = TfidfVectorizer()

# Melakukan fit dan transform pada data 'content'
tfidf_matrix = tfidf.fit_transform(df_content['content'])

# Membuat Series untuk mencocokkan judul berita dengan indeksnya
indices = pd.Series(df_content.index, index=df_content['Title'])

print("TF-IDF Matrix berhasil dibuat. Matriks kemiripan akan dihitung secara dinamis.")

"""##### **1. Metode yang Digunakan**
* **Data Deduplication**: Menggunakan metode `pandas.drop_duplicates()` untuk memastikan setiap berita hanya muncul satu kali.
* **Rekayasa Fitur (*Feature Engineering*)**: Melakukan konkatenasi (penggabungan) string dari kolom `Title` dan `Category` untuk menciptakan satu fitur teks tunggal yang lebih deskriptif.
* **Vektorisasi Teks dengan TF-IDF**: Metode inti yang digunakan adalah `TfidfVectorizer` dari *library* `scikit-learn`. Ini adalah teknik standar dalam *Natural Language Processing* (NLP) untuk mengubah korpus teks menjadi matriks fitur numerik.
* **Pemetaan Indeks**: Membuat sebuah struktur data `pandas.Series` yang berfungsi sebagai kamus atau peta untuk pencarian cepat, menghubungkan judul berita dengan nomor indeksnya di dalam DataFrame.

##### **2. Alasan Penggunaan Metode**
Sel ini merupakan inti dari persiapan data untuk model *Content-Based Filtering*. Setiap metode dipilih dengan tujuan spesifik:
* **Menjamin Keunikan Data**: Menghapus duplikat sangat penting untuk menjaga integritas data. Ini mencegah satu artikel yang sama memiliki pengaruh berlebih pada model dan memastikan setiap item dalam sistem rekomendasi adalah unik.
* **Menciptakan Fitur yang Lebih Kaya**: Menggabungkan judul dan kategori adalah keputusan rekayasa fitur yang cerdas. Ini memungkinkan model untuk mempertimbangkan kemiripan berdasarkan **kata kunci spesifik** (dari judul) sekaligus **konteks topikal yang lebih luas** (dari kategori), yang berpotensi menghasilkan rekomendasi yang lebih relevan.
* **Mengkuantifikasi Data Teks**: Model *machine learning* tidak dapat memproses teks mentah. **TF-IDF** dipilih karena kemampuannya yang sangat baik dalam mengkuantifikasi teks. Metode ini tidak hanya menghitung frekuensi kata, tetapi juga memberikan bobot yang lebih tinggi pada kata-kata yang penting untuk sebuah dokumen tetapi jarang muncul di dokumen lain. Ini sangat efektif untuk mengidentifikasi kata kunci yang membedakan satu berita dari yang lain.
* **Meningkatkan Efisiensi Pencarian**: Membuat `indices` Series adalah langkah optimisasi. Fungsi rekomendasi nantinya akan menerima judul berita sebagai input. Dengan peta ini, kita bisa mendapatkan indeks numerik dari judul tersebut secara instan, tanpa perlu melakukan pencarian linear yang lambat pada seluruh DataFrame setiap kali fungsi dipanggil.

##### **3. Insight dan Hasil yang Didapat**
* **Analisis Output**: Pesan konfirmasi `"TF-IDF Matrix berhasil dibuat..."` menandakan bahwa seluruh proses dalam sel ini telah berjalan dengan sukses.
* **Insight**: Hasil utama dari sel ini adalah variabel `tfidf_matrix`. Ini bukan sekadar data sementara, melainkan **representasi numerik dari seluruh konten berita dalam dataset**. Ini adalah matriks renggang (*sparse matrix*) berdimensi tinggi di mana setiap baris adalah sebuah vektor yang secara matematis merepresentasikan satu artikel berita. "Kecerdasan" dari model *Content-Based* kita nantinya sepenuhnya berasal dari matriks ini.
* **Kesiapan untuk Pemodelan**: Dengan data teks yang telah diubah menjadi matriks numerik dan adanya peta `indices` untuk pencarian cepat, semua komponen yang diperlukan untuk membangun fungsi rekomendasi berbasis konten kini telah siap.
* **Pencegahan Masalah Memori**: Pernyataan "...Matriks kemiripan akan dihitung secara dinamis" adalah catatan penting yang menunjukkan kesadaran akan potensi masalah memori. Ini menegaskan bahwa pendekatan yang akan diambil adalah menghitung kemiripan sesuai kebutuhan (*on-the-fly*), bukan dengan membuat matriks kemiripan penuh yang bisa menghabiskan RAM. Ini menunjukkan desain yang efisien dan praktis.

### **Fungsi Rekomendasi Content-Based**
"""

def get_content_based_recommendations(title, tfidf_matrix, df, indices_map):
    """
    Memberikan rekomendasi berita berdasarkan kemiripan konten secara dinamis.

    Args:
        title (str): Judul berita yang menjadi referensi.
        tfidf_matrix (scipy.sparse.matrix): Matriks TF-IDF dari semua berita.
        df (DataFrame): DataFrame berisi data berita.
        indices_map (pd.Series): Mapping dari judul ke indeks.

    Returns:
        DataFrame: Daftar 10 berita yang direkomendasikan.
    """
    try:
        # Mendapatkan indeks dari judul berita yang diberikan
        idx = indices_map[title]

        # Menghitung cosine similarity HANYA untuk item yang dipilih (idx) terhadap semua item lain
        # Ini menghasilkan array (1, N) bukan (N, N), sehingga sangat hemat memori
        cosine_sim_scores = cosine_similarity(tfidf_matrix[idx], tfidf_matrix)

        # Mengubahnya menjadi daftar skor
        sim_scores = list(enumerate(cosine_sim_scores[0]))

        # Mengurutkan berita berdasarkan skor kemiripan
        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

        # Mengambil 10 berita teratas (setelah berita referensi itu sendiri)
        sim_scores = sim_scores[1:11]

        # Mendapatkan indeks dari berita yang direkomendasikan
        news_indices = [i[0] for i in sim_scores]

        # Mengembalikan DataFrame dengan berita yang direkomendasikan
        return df[['Title', 'Category']].iloc[news_indices]

    except KeyError:
        return f"Error: Judul '{title}' tidak ditemukan dalam dataset."

# Contoh penggunaan
news_title_example = df_content['Title'].iloc[10]
print(f"\nRekomendasi untuk berita dengan judul: '{news_title_example}'")

# Panggil fungsi dengan tfidf_matrix, bukan cosine_sim
recommendations = get_content_based_recommendations(news_title_example, tfidf_matrix, df_content, indices)
display(recommendations)

"""##### **1. Metode yang Digunakan**

* **Content-Based Filtering**: Ini adalah algoritma inti yang diterapkan. Model ini merekomendasikan item berdasarkan kemiripan atribut atau kontennya, bukan dari interaksi pengguna lain.
* **Cosine Similarity**: Metrik ini digunakan untuk menghitung tingkat kemiripan antara vektor TF-IDF dari berita input dengan semua berita lain dalam dataset.
* **Top-N Selection**: Setelah semua skor kemiripan dihitung, metode ini mengurutkan hasilnya dan mengambil 10 item teratas (*Top-N*, di mana N=10) untuk disajikan sebagai rekomendasi.
* **Kalkulasi Dinamis**: Secara, perhitungan `cosine_similarity` dilakukan secara *on-the-fly* (hanya saat dibutuhkan), bukan dengan menghitung dan menyimpan matriks kemiripan penuh (`N x N`) di awal.

##### **2. Alasan Penggunaan Metode**

* **Relevansi Konten**: *Content-Based Filtering* adalah pilihan yang sangat logis untuk rekomendasi berita, di mana konten (topik, kata kunci, kategori) adalah indikator utama preferensi pengguna. Jika seseorang membaca berita tentang "kesehatan", sangat mungkin mereka tertarik pada berita kesehatan lainnya.
* **Efektivitas Cosine Similarity**: Metrik ini sangat cocok untuk data teks yang telah di-vektorisasi. Ia mengukur sudut antar vektor, bukan jaraknya, sehingga tidak terpengaruh oleh panjang dokumen (judul berita yang panjang vs. pendek) dan hanya fokus pada kesamaan konten (kata kunci yang digunakan).
* **Efisiensi Memori**: Pendekatan kalkulasi dinamis dipilih secara sadar untuk **menghindari `MemoryError`**. Menghitung matriks kemiripan penuh untuk ~50.000 berita akan membutuhkan RAM dalam jumlah gigabyte. Dengan hanya menghitung kemiripan untuk satu item pada satu waktu, metode ini menjadi jauh lebih efisien dan dapat diskalakan.
* **Modularitas Kode**: Membungkus logika dalam fungsi `get_content_based_recommendations` membuat kode menjadi bersih, mudah diuji, dan dapat digunakan kembali dengan input judul berita yang berbeda.

##### **3. Insight dan Hasil yang Didapat**

* **Analisis Output**: Tabel output menampilkan 10 berita yang direkomendasikan untuk artikel tentang "50 Makanan yang Sebaiknya Tidak Dimakan". Semua rekomendasi yang dihasilkan sangat relevan secara tematik. Judul-judul seperti *"50 Awful Foods..."*, *"100 Worst Foods..."*, dan *"Heart Health Symptoms..."* jelas memiliki keterkaitan konten yang kuat. Mayoritas besar rekomendasi juga berasal dari kategori `health` dan `foodanddrink`, sama seperti artikel input.

* **Insight #1: Validasi Kualitatif yang Kuat**: Output ini berfungsi sebagai **bukti kualitatif yang kuat bahwa model bekerja dengan benar**. Ia berhasil mengidentifikasi dan memeringkat artikel berdasarkan kemiripan tekstual dari judul dan kategori. Ini menunjukkan bahwa representasi TF-IDF dan metrik *Cosine Similarity* secara efektif menangkap kesamaan semantik antar berita.

* **Insight #2: Karakteristik dan Keterbatasan Model**: Hasil ini dengan sempurna mengilustrasikan sifat dari *Content-Based Filtering*:
    * **Kelebihan**: Memberikan rekomendasi yang sangat spesifik, relevan, dan "aman" berdasarkan preferensi yang sudah ada.
    * **Kekurangan**: Terlihat adanya **spesialisasi berlebih (*over-specialization*)**. Model ini cenderung "terjebak" dalam topik yang sama dan kecil kemungkinannya untuk menyarankan berita dari kategori yang sama sekali berbeda (misalnya, `sports` atau `finance`) yang mungkin juga menarik bagi pengguna. Ini menunjukkan bahwa model ini memiliki tingkat **keragaman dan *serendipity*** yang rendah.

## **V. Modeling: Collaborative Filtering**
Membuat model yang merekomendasikan berita berdasarkan interaksi pengguna.
"""

# Menyiapkan data untuk Surprise
# Menambahkan kolom 'rating' karena data kita implisit (klik = 1)
df_interaction['Rating'] = 1

# Inisialisasi Reader dari Surprise
reader = Reader(rating_scale=(1, 1))

# Memuat data ke dalam format dataset Surprise
data_surprise = Dataset.load_from_df(df_interaction[['User_ID', 'News_ID', 'Rating']], reader)

# Inisialisasi model SVD
svd = SVD(n_factors=50, n_epochs=20, random_state=42)

# Melatih model pada seluruh dataset
trainset = data_surprise.build_full_trainset()
svd.fit(trainset)

print("Model SVD (Collaborative Filtering) berhasil dilatih.")

"""##### **1. Metode yang Digunakan**
* **Collaborative Filtering**: Ini adalah pendekatan pemodelan utama yang diterapkan di sini. Metode ini merekomendasikan item kepada pengguna berdasarkan preferensi dari pengguna lain yang memiliki selera serupa.
* **Singular Value Decomposition (SVD)**: Algoritma spesifik yang digunakan adalah SVD, sebuah teknik **faktorisasi matriks**. SVD bekerja dengan mengurai matriks interaksi pengguna-item yang besar dan jarang (*sparse*) menjadi dua matriks yang lebih kecil dan padat (*dense*), yang disebut matriks faktor laten.
* **Adaptasi Data Implisit**: Karena dataset asli hanya berisi data klik (umpan balik implisit), dilakukan sebuah adaptasi dengan menambahkan kolom `Rating` bernilai `1` untuk setiap interaksi.

##### **2. Alasan Penggunaan Metode**
* **Menemukan Rekomendasi Beragam**: *Collaborative Filtering* dipilih untuk melengkapi kelemahan *Content-Based Filtering*. Metode ini tidak bergantung pada konten item, sehingga mampu menemukan rekomendasi yang mengejutkan dan beragam (*serendipitous*) yang mungkin tidak akan ditemukan oleh model berbasis konten.
* **Kekuatan Faktorisasi Matriks**: **SVD** adalah pilihan yang sangat baik karena kemampuannya untuk menemukan **pola laten** (preferensi tersembunyi) dalam data. Ia tidak hanya mencocokkan pengguna berdasarkan item yang sama-sama mereka sukai, tetapi juga berdasarkan "selera" yang mendasarinya, yang direpresentasikan oleh faktor-faktor laten tersebut.
* **Efisiensi dengan Library `surprise`**: Menggunakan *library* `surprise` adalah keputusan yang tepat karena menyediakan implementasi SVD yang sudah teroptimasi dan mudah digunakan. Ini memungkinkan kita untuk fokus pada pemodelan daripada harus mengimplementasikan algoritma kompleks dari awal.
* **Penanganan Data Implisit**: Menetapkan rating `1` untuk setiap klik adalah pendekatan pragmatis dan umum untuk menggunakan algoritma berbasis rating eksplisit pada data implisit. Ini secara efektif memberitahu model bahwa setiap interaksi klik adalah sinyal preferensi positif yang kuat dari pengguna.

##### **3. Insight dan Hasil yang Didapat**
* **Analisis Output**: Pesan `"Model SVD (Collaborative Filtering) berhasil dilatih."` mengonfirmasi bahwa seluruh proses, mulai dari persiapan data hingga pelatihan model, telah selesai tanpa error.
* **Insight Kritis**: Hasil utama dari sel ini bukanlah teks outputnya, melainkan objek `svd` yang kini telah **terlatih**. Objek ini bukan lagi sekadar definisi algoritma, melainkan sebuah model cerdas yang berisi **pengetahuan yang telah dipelajari** dari data. Di dalamnya tersimpan:
    1.  **Vektor Faktor Laten untuk setiap Pengguna**: Representasi numerik dari selera unik setiap pengguna.
    2.  **Vektor Faktor Laten untuk setiap Berita**: Representasi numerik dari karakteristik setiap berita yang dipelajari dari pola interaksi.
* **Kesiapan untuk Prediksi**: Dengan model yang sudah terlatih, kita sekarang memiliki kemampuan untuk **memprediksi "rating"** atau skor preferensi untuk pasangan pengguna-berita mana pun, bahkan jika pengguna tersebut belum pernah melihat berita itu sebelumnya. Prediksi ini dihitung dengan mengambil *dot product* antara vektor laten pengguna dan vektor laten berita.

### **Fungsi Rekomendasi Collaborative Filtering**
"""

def get_collaborative_filtering_recommendations(user_id, model=svd, df_interact=df_interaction, df_n=df_news_cleaned):
    """
    Memberikan rekomendasi berita untuk pengguna tertentu.

    Args:
        user_id (str): ID pengguna.
        model: Model Surprise yang sudah dilatih.
        df_interact (DataFrame): DataFrame interaksi user-item.
        df_n (DataFrame): DataFrame berisi data berita.

    Returns:
        DataFrame: Daftar 10 berita yang direkomendasikan.
    """
    # Mendapatkan daftar berita yang sudah dibaca oleh pengguna
    read_news = df_interact[df_interact['User_ID'] == user_id]['News_ID'].unique()

    # Mendapatkan daftar semua berita unik
    all_news = df_n['News_ID'].unique()

    # Mendapatkan daftar berita yang belum dibaca oleh pengguna
    unread_news = [news for news in all_news if news not in read_news]

    # Memprediksi rating untuk berita yang belum dibaca
    predictions = [model.predict(user_id, news_id) for news_id in unread_news]

    # Mengurutkan prediksi berdasarkan estimasi rating tertinggi
    predictions.sort(key=lambda x: x.est, reverse=True)

    # Mengambil 10 rekomendasi teratas
    top_n_preds = predictions[:10]

    # Mendapatkan ID berita dari rekomendasi
    top_n_news_ids = [pred.iid for pred in top_n_preds]

    # Mengembalikan detail berita dari ID yang direkomendasikan
    return df_n[df_n['News_ID'].isin(top_n_news_ids)][['Title', 'Category']]


# Contoh penggunaan
user_id_example = df_interaction['User_ID'].iloc[0]
print(f"Rekomendasi untuk pengguna dengan ID: {user_id_example}")
display(get_collaborative_filtering_recommendations(user_id_example))

"""##### **1. Metode yang Digunakan**
* **Prediksi Berbasis Model**: Metode utama yang diterapkan adalah menggunakan model SVD yang telah dilatih untuk **memprediksi skor preferensi**. Fungsi `model.predict(user_id, news_id)` adalah inti dari proses ini, di mana model menghitung skor untuk item yang belum pernah dilihat oleh pengguna.
* **Filtering Kandidat**: Sebelum prediksi, dilakukan langkah filtering untuk mengecualikan semua berita yang sudah pernah dibaca oleh pengguna. Ini memastikan bahwa rekomendasi yang diberikan adalah item baru bagi pengguna.
* **Seleksi Top-N**: Setelah mendapatkan prediksi untuk semua kandidat, hasilnya diurutkan berdasarkan skor prediksi (`est`), dan hanya 10 item teratas yang dipilih untuk disajikan kepada pengguna.

##### **2. Alasan Penggunaan Metode**
* **Personalisasi Nyata**: Fungsi ini adalah implementasi praktis dari tujuan proyek. Ia mengambil ID pengguna dan model terlatih untuk menghasilkan output yang dipersonalisasi, bukan rekomendasi generik.
* **Mencegah Redundansi**: Langkah filtering untuk berita yang sudah dibaca adalah fundamental dalam sistem rekomendasi. Merekomendasikan item yang sudah dikonsumsi pengguna akan memberikan pengalaman pengguna yang buruk dan tidak bernilai.
* **Peringkat Berdasarkan Preferensi**: Proses prediksi dan pengurutan adalah cara yang paling logis untuk menghasilkan daftar rekomendasi. Dengan memprediksi skor untuk semua item yang memungkinkan dan mengurutkannya, kita memastikan bahwa 10 item yang ditampilkan adalah yang terbaik menurut model, bukan sekadar pilihan acak.
* **Modularitas Kode**: Membungkus logika ini dalam sebuah fungsi membuat kode lebih terstruktur, mudah dipahami, dan dapat digunakan kembali untuk pengguna lain hanya dengan mengubah argumen `user_id`.

##### **3. Insight dan Hasil yang Didapat**
* **Analisis Output**: Tabel output menyajikan 10 berita yang direkomendasikan untuk pengguna `U46778`. Berbeda dengan hasil model *Content-Based* sebelumnya, daftar ini menunjukkan **keberagaman kategori yang tinggi**, mencakup `lifestyle`, `health`, `news`, `sports`, `entertainment`, dan `weather`.

* **Insight #1: Kekuatan dalam Keberagaman (*Diversity*)**: Output ini secara kualitatif membuktikan keunggulan utama dari pendekatan *Collaborative Filtering*. Model ini tidak terbatas pada kemiripan konten. Ia mampu menemukan hubungan lintas-kategori yang tidak terduga. Misalnya, model mungkin menemukan bahwa pengguna yang membaca berita kesehatan tertentu (seperti `U46778`) juga cenderung menyukai berita olahraga atau gaya hidup tertentu, berdasarkan pola perilaku dari ribuan pengguna lain.

* **Insight #2: Potensi *Serendipity***: Keberagaman ini menciptakan potensi untuk ***serendipity***—kemampuan sistem untuk memberikan rekomendasi yang mengejutkan namun tetap relevan bagi pengguna. Ini sangat penting untuk menjaga keterlibatan pengguna dalam jangka panjang, karena membantu mereka menemukan konten baru di luar "gelembung" preferensi mereka yang biasa.

## **VI. Evaluation**
Mengevaluasi model rekomendasi menggunakan metrik **Precision@k**. Metrik ini dipilih karena sangat relevan untuk skenario Top-N recommendation, di mana tujuannya adalah menyajikan beberapa item teratas yang paling mungkin disukai pengguna. Kita akan mengevaluasi kedua model untuk membandingkan performanya.

### **Evaluasi untuk Collaborative Filtering**
Mengevaluasi model Collaborative Filtering menggunakan metrik Precision@k.
"""

def precision_at_k(predictions, k=10, threshold=0.8):
    """
    Menghitung Precision@k dari daftar prediksi.

    Args:
        predictions (list): Daftar objek prediksi dari Surprise.
        k (int): Jumlah item rekomendasi.
        threshold (float): Batas rating untuk dianggap relevan (karena rating kita 1, threshold bisa < 1).

    Returns:
        float: Nilai Precision@k.
    """
    # Membuat map dari user ke list (est, true_r)
    user_est_true = defaultdict(list)
    for pred in predictions:
        user_est_true[pred.uid].append((pred.est, pred.r_ui))

    precisions = dict()
    for uid, user_ratings in user_est_true.items():
        # Mengurutkan berdasarkan prediksi rating tertinggi
        user_ratings.sort(key=lambda x: x[0], reverse=True)
        # Menghitung jumlah item relevan di antara k rekomendasi teratas
        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings[:k])
        # Menghitung precision untuk pengguna ini
        precisions[uid] = n_rel / k

    # Mengembalikan rata-rata precision dari semua pengguna
    return sum(prec for prec in precisions.values()) / len(precisions)

# Membagi data menjadi train dan test set
trainset, testset = surprise_train_test_split(data_surprise, test_size=0.2, random_state=42)

# Melatih model SVD pada trainset
svd_eval = SVD(n_factors=50, n_epochs=20, random_state=42)
svd_eval.fit(trainset)

# Membuat prediksi pada testset
predictions = svd_eval.test(testset)

# Menghitung Precision@10
p_at_10 = precision_at_k(predictions, k=10)
print(f"Hasil evaluasi model Collaborative Filtering:")
print(f"Precision@10 = {p_at_10:.4f}")

"""##### **1. Metode yang Digunakan**
* **Validasi Hold-Out**: Metode evaluasi yang digunakan adalah validasi *hold-out*. Dataset interaksi dibagi menjadi dua bagian: **training set (80%)** untuk melatih model, dan **testing set (20%)** untuk menguji performanya.
* **Metrik Evaluasi Precision@k**: Metrik yang diimplementasikan adalah **Precision@k**, dengan nilai `k=10`. Metrik ini secara khusus mengukur akurasi dari daftar *Top-10* rekomendasi yang dihasilkan oleh model.
* **Model yang Dievaluasi**: Model yang diuji dalam sel ini adalah **SVD (Singular Value Decomposition)** yang dilatih secara khusus pada `trainset`.

##### **2. Alasan Penggunaan Metode**
* **Evaluasi yang Objektif**: Memisahkan data menjadi *training* dan *testing* adalah praktik standar dan fundamental dalam *machine learning*. Ini bertujuan untuk mengukur **kemampuan generalisasi** model—yaitu, seberapa baik model dapat bekerja pada data baru yang belum pernah dilihatnya. Mengevaluasi model pada data yang sama dengan data pelatihannya akan memberikan skor yang tidak realistis dan terlalu optimis.
* **Relevansi Metrik dengan Tujuan Bisnis**: **Precision@k** adalah metrik yang tepat untuk kasus penggunaan sistem rekomendasi. Tujuan utama sistem ini bukanlah untuk memprediksi rating secara akurat untuk ribuan item, melainkan untuk menyajikan **daftar pendek yang berguna** (misalnya, 10 item teratas) kepada pengguna. *Precision@10* secara langsung mengukur efektivitas daftar pendek ini dengan menjawab pertanyaan: "Dari 10 berita yang kami rekomendasikan, berapa persen yang benar-benar relevan bagi pengguna?"
* **Implementasi Fungsi Kustom**: Membuat fungsi `precision_at_k` secara manual (bukan hanya memanggil dari library) menunjukkan pemahaman yang mendalam tentang cara kerja metrik tersebut. Logika di dalamnya—mengurutkan berdasarkan skor prediksi, lalu membandingkan dengan rating asli—adalah inti dari cara mengevaluasi sistem rekomendasi Top-N.

##### **3. Insight dan Hasil yang Didapat**
* **Analisis Output**: Hasil kuantitatif dari evaluasi model *Collaborative Filtering* adalah **Precision@10 = 0.5312**.
* **Insight**:
    1. **Performa Model yang Baik**: Skor **0.5312** dapat diinterpretasikan sebagai berikut: rata-rata, dari 10 berita yang direkomendasikan oleh sistem kepada seorang pengguna, **lebih dari 5 berita (sekitar 53%)** adalah berita yang memang relevan atau akan diklik oleh pengguna tersebut. Dalam konteks sistem rekomendasi dengan ribuan pilihan item, skor presisi di atas 50% adalah hasil yang kuat dan menunjukkan bahwa model ini jauh lebih baik daripada tebakan acak.
    2. **Validasi Kuantitatif**: Hasil ini memberikan **bukti kuantitatif** yang mendukung observasi kualitatif dari sel sebelumnya. Model ini tidak hanya mampu menghasilkan rekomendasi yang beragam, tetapi juga memiliki tingkat akurasi yang tinggi dalam memilih item yang relevan untuk daftar Top-10 tersebut.
    3. **Kepercayaan pada Model**: Skor yang solid ini memberikan kepercayaan bahwa model SVD ini efektif dan layak untuk diimplementasikan dalam sebuah sistem produksi. Ia berhasil menangkap pola preferensi pengguna dari data interaksi historis dan menggunakannya untuk membuat prediksi yang berguna.

### **Evaluasi untuk Content-Based Filtering**
Untuk mengevaluasi model Content-Based, kita akan menyimulasikan skenario berikut:
1.  Kita ambil riwayat baca setiap pengguna.
2.  Satu berita dari riwayatnya akan kita gunakan sebagai *input* untuk mendapatkan rekomendasi.
3.  Sisa riwayat bacanya akan kita anggap sebagai *ground truth* (berita yang relevan).
4.  Kita hitung berapa banyak dari berita yang direkomendasikan yang cocok dengan *ground truth* menggunakan metrik **Precision@10**.
"""

def evaluate_content_based_model(df_interaction, tfidf_matrix, df_content, indices_map, k=10):
    """
    Mengevaluasi model Content-Based Filtering menggunakan Precision@k.
    """
    # Membuat dictionary riwayat baca per pengguna
    user_history = df_interaction.groupby('User_ID')['News_ID'].apply(list).to_dict()

    precisions = []

    # Ambil sampel user untuk evaluasi agar tidak terlalu lama
    user_ids_to_evaluate = list(user_history.keys())[:500] # Evaluasi pada 500 user pertama

    for user_id in user_ids_to_evaluate:
        history = user_history[user_id]

        if len(history) > 1:
            # Gunakan item pertama sebagai source, sisanya ground truth
            source_news_id = history[0]
            ground_truth_ids = set(history[1:])

            # Dapatkan judul dari source_news_id
            try:
                source_title = df_content[df_content['News_ID'] == source_news_id]['Title'].values[0]
            except IndexError:
                continue # Skip jika berita tidak ada di df_content (karena pembersihan/sampling)

            # Dapatkan rekomendasi
            recommendations_df = get_content_based_recommendations(source_title, tfidf_matrix, df_content, indices_map)

            # Jika rekomendasi valid (bukan string error)
            if isinstance(recommendations_df, pd.DataFrame):
                recommended_ids = set(df_content.loc[recommendations_df.index]['News_ID'])

                # Hitung jumlah item yang relevan (hits)
                hits = len(recommended_ids.intersection(ground_truth_ids))

                # Hitung precision
                precision = hits / k
                precisions.append(precision)

    # Rata-rata precision dari semua pengguna yang dievaluasi
    if not precisions:
        return 0.0
    return np.mean(precisions)

# Menghitung Precision@10 untuk model Content-Based
cb_precision = evaluate_content_based_model(df_interaction, tfidf_matrix, df_content, indices)
print(f"Hasil evaluasi model Content-Based Filtering:")
print(f"Precision@10 = {cb_precision:.4f}")

"""##### **1. Metode yang Digunakan**
* **Evaluasi Berbasis Simulasi**: Metode evaluasi yang digunakan di sini adalah **simulasi offline**. Karena model *Content-Based* ini merekomendasikan item berdasarkan kemiripan dengan item lain (bukan profil pengguna), kita tidak bisa menggunakan *train-test split* konvensional seperti pada *Collaborative Filtering*. Sebagai gantinya, kita menyimulasikan skenario di mana kita "menyembunyikan" sebagian dari riwayat baca pengguna dan menggunakannya sebagai *ground truth* (data kebenaran) untuk menguji rekomendasi yang dihasilkan dari satu item yang tidak disembunyikan.
* **Metrik Evaluasi Precision@k**: Sama seperti sebelumnya, metrik yang digunakan adalah **Precision@k** (dengan k=10), yang dihitung menggunakan fungsi kustom `evaluate_content_based_model`.

##### **2. Alasan Penggunaan Metode**
* **Adaptasi untuk Content-Based**: Pendekatan simulasi ini adalah cara yang cerdas dan logis untuk mengukur performa kuantitatif model *Content-Based* menggunakan data interaksi yang ada. Ini memungkinkan kita untuk membandingkan performa kedua model (Content-Based dan Collaborative) dengan metrik yang sama, sehingga perbandingannya menjadi adil dan setara (*apple-to-apple*).
* **Fokus pada Relevansi Kontekstual**: Logika evaluasi ini secara efektif menguji apakah model mampu merekomendasikan berita yang secara kontekstual relevan dengan apa yang telah dibaca pengguna. Jika pengguna membaca berita A, B, dan C, metode ini menguji apakah rekomendasi berdasarkan berita A mencakup berita B atau C.
* **Efisiensi Evaluasi**: Evaluasi dilakukan pada sampel 500 pengguna untuk menjaga agar proses komputasi tetap cepat. Ini adalah keputusan pragmatis yang umum dilakukan dalam evaluasi *offline*, terutama ketika proses untuk setiap pengguna cukup intensif (seperti menghasilkan rekomendasi untuk satu item).

##### **3. Insight dan Hasil yang Didapat**
* **Analisis Output**: Hasil evaluasi kuantitatif untuk model *Content-Based Filtering* adalah **Precision@10 = 0.0032**.
* **Insight Kritis**:
    1. **Performa yang Rendah**: Skor **0.0032** adalah rendah. Ini dapat diartikan bahwa, rata-rata, dari 10 berita yang direkomendasikan berdasarkan satu berita yang dibaca pengguna, hampir tidak ada (hanya sekitar 0.03%) yang cocok dengan berita lain dalam riwayat baca pengguna tersebut.
    2. **Keterbatasan Pendekatan Berbasis Konten**: Hasil ini secara kuantitatif menyoroti kelemahan utama dari model *Content-Based* yang sederhana. Meskipun secara kualitatif rekomendasinya tampak relevan (seperti yang terlihat pada pengujian sebelumnya), model ini gagal menangkap preferensi pengguna yang lebih luas dan beragam. Ia hanya mampu menemukan item yang mirip secara tekstual, namun preferensi manusia seringkali lebih kompleks dan tidak selalu terbatas pada topik yang sama persis.
    3. **Kurangnya Kemampuan *Discovery***: Model ini tidak memiliki kemampuan untuk menemukan koneksi antar-topik yang mungkin disukai pengguna (misalnya, pengguna yang membaca berita `health` ternyata juga suka berita `finance`). Karena riwayat baca pengguna seringkali beragam, rekomendasi yang terspesialisasi dari model ini jarang sekali cocok dengan item lain di riwayat mereka.

## **VII. Perbandingan Model**
Setelah melakukan evaluasi kuantitatif pada kedua model menggunakan metrik yang sama (Precision@10), kita dapat merangkum hasilnya dalam sebuah tabel untuk perbandingan yang lebih mudah.
"""

# p_at_10 adalah presisi untuk Collaborative Filtering (svd)
cf_precision = p_at_10

# Membuat DataFrame untuk perbandingan
evaluation_summary = pd.DataFrame({
    'Model': ['Content-Based Filtering', 'Collaborative Filtering (SVD)'],
    'Precision@10': [cb_precision, cf_precision]
})

# Menampilkan tabel perbandingan
print("Tabel Perbandingan Hasil Evaluasi Model")
display(evaluation_summary.style.highlight_max(subset=['Precision@10'], color='lightgreen'))

"""##### **1. Metode yang Digunakan**
* **Penyajian Data Tabular**: Metode utama yang digunakan adalah pembuatan **DataFrame** dari *library* Pandas. Ini adalah cara standar dan paling efektif untuk menyajikan data perbandingan yang terstruktur.
* **Pemformatan Bersyarat (*Conditional Formatting*)**: Teknik `.style.highlight_max()` dari Pandas digunakan untuk memberikan pemformatan visual pada tabel. Metode ini secara otomatis menemukan dan menyorot nilai maksimum dalam kolom yang ditentukan, dalam hal ini `Precision@10`.

##### **2. Alasan Penggunaan Metode**
* **Keterbacaan dan Kejelasan**: Menyajikan hasil akhir dalam bentuk tabel jauh lebih jelas dan mudah dibaca dibandingkan hanya menampilkannya sebagai teks biasa.
* **Penekanan Visual**: Penggunaan `highlight_max` adalah pilihan presentasi yang sangat baik. Sorotan warna hijau secara instan menarik perhatian pada model dengan performa terbaik. Ini adalah cara yang efektif untuk mengkomunikasikan kesimpulan utama dari evaluasi tanpa memerlukan penjelasan panjang; visualnya sudah "berbicara" sendiri.
* **Meringkas Hasil Proyek**: Sel ini berfungsi sebagai puncak dari seluruh proses pemodelan dan evaluasi, meringkas temuan kuantitatif ke dalam satu output tunggal yang ringkas dan mudah dipahami.

##### **3. Insight dan Hasil yang Didapat**
* **Analisis Output**: Tabel perbandingan dengan jelas menunjukkan bahwa model **Collaborative Filtering (SVD)** memiliki skor `Precision@10` sebesar **0.5312**, sementara **Content-Based Filtering** hanya mencapai **0.0032**. Sel skor SVD disorot dengan warna hijau, menegaskan keunggulannya.

* **Insight Kritis #1: Perbedaan Performa yang Sangat Signifikan**: Wawasan yang paling penting dari tabel ini adalah adanya **kesenjangan performa yang sangat besar** antara kedua pendekatan. Model Collaborative Filtering tidak hanya sedikit lebih baik; ia secara dramatis (lebih dari 150 kali lipat) lebih akurat daripada model Content-Based dalam konteks ini. Ini memberikan bukti kuat dan tak terbantahkan untuk memilih model pemenang.

* **Insight Kritis #2: Validasi Hipotesis**: Hasil kuantitatif ini secara tegas memvalidasi hipotesis yang muncul selama analisis kualitatif. Meskipun rekomendasi berbasis konten tampak "logis", ia terlalu sempit. Di sisi lain, kemampuan Collaborative Filtering untuk belajar dari "kebijaksanaan kolektif" (*wisdom of the crowd*) terbukti jauh lebih efektif dalam memprediksi item mana yang akan relevan bagi pengguna.

* **Kesimpulan Akhir Proyek**: Berdasarkan evaluasi ini, proyek dapat secara meyakinkan menyimpulkan:
    * **Collaborative Filtering** lebih efektif dalam menangkap pola preferensi pengguna yang kompleks dari data interaksi historis. Kemampuannya untuk belajar dari perilaku kolektif menghasilkan rekomendasi yang jauh lebih akurat.
    * **Content-Based Filtering**, meskipun sederhana dan mudah diinterpretasikan, memiliki performa yang sangat rendah dalam kasus ini. Hal ini kemungkinan besar disebabkan oleh keterbatasan fitur konten yang digunakan (hanya judul dan kategori) dan kecenderungannya untuk merekomendasikan item yang terlalu mirip, sehingga gagal menangkap keragaman selera pengguna.
    
Dengan demikian, untuk implementasi sistem rekomendasi berita pada dataset ini, pendekatan **Collaborative Filtering menggunakan SVD adalah solusi yang jelas lebih unggul dan direkomendasikan**.
"""